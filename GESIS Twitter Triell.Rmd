---
title: "Twitter communication with candidates for Bundeskanzler:in"
author: Melanie Dietz, Julia Lück-Benz, Carina Weinmann, Lorenz Biberstein
date: "`r format(Sys.Date(), '%d %b %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
# Global setup for all R chunks in this file
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE) # for now all R chunks are included, can be taken out here later
```

```{r libraries, include = FALSE}
library(rtweet)
library(ggplot2)
library(knitr) # for generating tables in markdown
library(janitor) # for creating tables
library(tm) # Für text mining
library(tidytext) # for text mining
library(data.table)
library(quanteda) # Quantitative Analysis of Textual Data 
library(quanteda.textstats)
library(syuzhet)
library(dplyr) # For topic modeling
library(topicmodels)
library(udpipe) # for german lemmatization
library(plotly)
library(spacyr) # Für Lemmatisierung
library(tidyr)
library(stringr) # Working with strings
library(plotly) # for interactive figures
library(wordcloud2)
```

# Introduction & research question

This is a group work of group \#2 in the GESIS fall school "Introduction to Computational Social Sciences with Applications in R".

The aim of this project is to analyse tweets surrounding the election of the Bundeskanzler:in in Germany, focusing on the three candidates Annalena Baerbock (Grüne), Olaf Scholz (SPD) and Armin Laschet (CDU). We tried to answer the following broad research question:

> *What does the Twitter discussion about the three candidates in the context of the ARD triell look like?*

# Getting Twitter data

First, we loaded data from Twitter using the `rtweet` package[^1]. (Another possibility would be to use the `academictwitteR`[^2], but we could not get it to work in the first tries, so stuck to `rtweet`.

[^1]: <https://cran.r-project.org/web/packages/rtweet/rtweet.pdf>

[^2]: <https://github.com/cjbarrie/academictwitteR>, <https://cran.r-project.org/web/packages/academictwitteR/index.html>

```{r twitter_token, include = FALSE}
# source("twitter_token.R", local = knitr::knit_global())
# load twitter tokens from separate file (not shared on github for privacy reasons)
```

As two persons from our group already had access to the Twitter API, we collected data twice:

1.  Using simply the query "Triell" (no hashtag), collecting as many tweets as possible
2.  Using separate queries for the candidates and using a hashtag with "Triell"; e.g. "`#triell AND baerbock`"

Both queries were limited to exclude retweets and only search for German language `include_rts = FALSE, lang = "de"`.

```{r get_tweets_save_large, include = FALSE}
# going for the big guns: trying to download 300k tweets
# 
# triell_large <- rtweet::search_tweets("#Triell", # Search tweets with search phrase "Triell"
#                          include_rts = FALSE, # do not include retweets
#                          lang = "de", # Only tweets from Twitter users with German language
#                          retryonratelimit = TRUE,
#                          n = 300000
#                          )
# 
# # Save data as csv
#  write_as_csv(triell,
#               "data/triell_2021-09-16_large.csv",
#               prepend_ids = TRUE,
#               na = "",
#               fileEncoding = "UTF-8"
#               )

# saveRDS(triell_large, "triell_large.rds")
```

```{r import_csv, include = FALSE}
triell_large <- read.csv("data/triell_2021-09-16_large.csv")

triell_b <- read.csv("data/triell_b_2021-09-15.csv")
triell_l <- read.csv("data/triell_l_2021-09-15.csv")
triell_s <- read.csv("data/triell_s_2021-09-15.csv")
```

The three separate dataframes for the three candidates were then combined together to get one combined dataset called `triell_all` (after adding an identifier variable called `candidate` to distinguish from which query the data comes).

```{r combine_separate_candidates_to_all}
triell_b <- dplyr::mutate(triell_b, candidate = "Baerbock")
triell_l <- dplyr::mutate(triell_l, candidate = "Laschet")
triell_s <- dplyr::mutate(triell_s, candidate = "Scholz")

triell_all <- rbind(triell_b, triell_l, triell_s)
```

This gave us two dataframes; one with the combined searched tweets of the candidates; called `triell_all` of `r count(triell_all)` entries and one large dataframe called `triell_large` of `r count(triell_large)` entries. *We subsequently usually ran our following analyses for both of these datasets as we found it interesting to compare the results based on the slightly different search queries and results (hashtag vs. no hashtag, searching for candidates' names specifically vs. only for "Triell").*

```{r structures, include = FALSE}
str(triell_all)

str(triell_large)
```

# Data cleaning and preparation

```{r cleaning}
triell_all$text <-  gsub("https\\S*", "", triell_all$text)
triell_all$text <-  gsub("@\\S*", "", triell_all$text) 
triell_all$text <-  gsub("amp", "", triell_all$text) 
triell_all$text <-  gsub("[\r\n]", "", triell_all$text)
# triell_all$text <-  gsub("[[:punct:]]", "", triell_all$text) # remove punctuation

triell_large$text <-  gsub("https\\S*", "", triell_large$text)
triell_large$text <-  gsub("@\\S*", "", triell_large$text) 
triell_large$text <-  gsub("amp", "", triell_large$text) 
triell_large$text <-  gsub("[\r\n]", "", triell_large$text)
# triell_large$text <-  gsub("[[:punct:]]", "", triell_large$text) # remove punctuation
```

## Add candidate identifiers

In the dataset `triell_all` we were able to identify candidates based on the Twitter query (e.g. `#triell AND baerbock`). But as it was possible to have multiple tweets naming not only one but multiple candidated, this identification was not very precise. For this reason, we added another variable searching for each candidate in the text of the Tweet. This gave us three separate variables showing us, whether each of the candidates was present in the tweets (variable `baerbock`, `laschet` and `scholz`).

These three variables were then combined to a new variable called `candidate_comb` to show the combination of the candidates' appearances in the tweets (e.g. Baerbock + Laschet, Baerbock + Scholz + Laschet etc.).

```{r candidate_id}
# Add identifier variable for appearance of candidates in tweets
triell_large <- triell_large %>%
  mutate(baerbock = ifelse(grepl("aerbock", text), 1, 0)) %>%
  mutate(laschet = ifelse(grepl("aschet", text), 10, 0)) %>%
  mutate(scholz = ifelse(grepl("olz", text), 100, 0))

# Combine id variables for all three candidates
triell_large <- triell_large %>%
  mutate(candidate_comb = baerbock + laschet + scholz)

# define labels to combination of appearance of candidates
triell_large$candidate_comb <- factor(triell_large$candidate_comb,
levels = c(1, 10, 100, 11, 101, 110, 111),
labels = c("Baerbock", "Laschet", "Scholz", "Baerbock + Laschet", "Baerbock + Scholz", "Laschet + Scholz", "Baerbock + Scholz + Laschet"))

# do the same for data "triell_all"

# Add identifier variable for appearance of candidates in tweets
triell_all <- triell_all %>%
  mutate(baerbock = ifelse(grepl("aerbock", text), 1, 0)) %>%
  mutate(laschet = ifelse(grepl("aschet", text), 10, 0)) %>%
  mutate(scholz = ifelse(grepl("olz", text), 100, 0))

# Combine id variables for all three candidates
triell_all <- triell_all %>%
  mutate(candidate_comb = baerbock + laschet + scholz)

# define labels to combination of appearance of candidates
triell_all$candidate_comb <- factor(triell_all$candidate_comb,
levels = c(1, 10, 100, 11, 101, 110, 111),
labels = c("Baerbock", "Laschet", "Scholz", "Baerbock + Laschet", "Baerbock + Scholz", "Laschet + Scholz", "Baerbock + Scholz + Laschet"))
```

## Adding dates and times

First we check the first and last dates of our twitter datasets and we see - not surprising - that both dataframes have their newest entries from September 15th (day of download) and oldest entries from September 7th (as you can only access tweets from a certain number of days back via the twitter API).

```{r, include = FALSE}
head(triell_all$created_at)
tail(triell_all$created_at)

head(triell_large$created_at)
tail(triell_large$created_at)
```

Next, we isolated the single days of the `created_at` for later usage.

```{r isolate_days}
triell_all$day <- substr(triell_all$created_at, 1, 10)

triell_large$day <- substr(triell_large$created_at, 1, 10)
```

We can also identify tweets which were only sent *during* the second Triell.

```{r}
### use only tweets during the Triell at RTL 

triell_large$month <- substr(triell_large$created_at, 1, 7)

triell_large$day <- substr(triell_large$created_at, 1, 10)

triell_large$time <- substr(triell_large$created_at, 12, 16)
                     
triellARD <- subset(triell_large, triell_large$day == "2021-09-12")

triellARDlive <- subset(triellARD, triellARD$time > "20:14")

ARDtriell <- subset(triellARDlive, triellARDlive$time < "21:55")
```

## Generate corpus & wordlists

For potential later analysis we also created separate objects isolating each word of the tweets, together with the users and tweet time (`created_at`).

```{r generate_wordlist_text}
triell_all_text <- triell_all %>%
  select(name, text, created_at) %>%
  unnest_tokens(word, text)
# new object, only words and names of twitter accounts

triell_large_text <- triell_large %>%
  select(name, text, created_at) %>%
  unnest_tokens(word, text)
# new object, only words and names of twitter accounts
```

Converting the dataframe to a wordlist gives us a new dataframe with `r count(triell_all_text)` entries (for `triell_all`) and `r count (triell_large_text)` (`triell_large`).

```{r, include = FALSE}
str(triell_all_text)

str(triell_large_text)
```

We could also convert the dataframes into corporae, but to properly do this we would need a German lemmatization word list.

```{r}
triell_all_corpus <- corpus(triell_all$text) %>% 
  tokens(.,remove_punct=TRUE,remove_numbers=TRUE,remove_symbols = TRUE) %>% 
  tokens_tolower() # %>% 
# tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed")

triell_large_corpus <- corpus(triell_large$text) %>% 
  tokens(.,remove_punct=TRUE,remove_numbers=TRUE,remove_symbols = TRUE) %>% 
  tokens_tolower() # %>% 
#  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed")
```

```{r, include = FALSE}
# str(triell_all_corpus)
# str(triell_large_corpus)
```

# Analysis

The aim of the data analysis was to find differences in the tweets, depending on who (which candidate, or which comibination of candidates is mentioned in the tweets) and on the time of sending the tweet.

## Sentiment analysis (using syuzhet in German)

### Differences by candidates' mentions

For this we get the sentiments of all the tweets, using the `Syuzhet` package, which can also be used with German (setting `lang <- "german"` and using `method = "nrc"` instead of `affin`, as `affin` is not available in German).

In the following code chunk we execute the sentiment analysis for the two datasets ("large" and "all").

```{r create_sentiments}
triell_all$sentiment <- get_sentiment(triell_all$text, method="nrc", language="german")
triell_all$sentiment[1:10]

triell_large$sentiment <- get_sentiment(triell_large$text, method="nrc", language="german")
triell_large$sentiment[1:10]
```

The mean of the sentiment values can then be calculated for each of the three candidates. To check the distribution of the sentiments by candidate we can graph the distribution as histograms. The sentiment scores range from `r min(triell_large$sentiment)` to `r max(triell_large$sentiment)` for the "large" dataset and `r min(triell_all$sentiment)` to `r max(triell_all$sentiment)` for the "all" dataset.

We can also visualise the distributions as histograms, as seen in the following figures.

```{r sentiment_histo}
triell_all %>%
  ggplot() +
  aes(x = sentiment) +
  geom_histogram(bins = 20L) +
  theme_minimal() +
  ggtitle("Sentiments of all tweets together (data 'all')")

triell_large %>%
  ggplot() +
  aes(x = sentiment) +
  geom_histogram(bins = 20L) +
  theme_minimal() +
  ggtitle("Sentiments of all tweets together (data 'large')")

triell_all %>%
  ggplot() +
  aes(x = sentiment) +
  geom_histogram(bins = 20L) +
  theme_minimal() +
  ggtitle("Sentiments of all tweets together, by candidate (data 'all')")  +
  facet_wrap(~ candidate)

triell_large %>%
  filter(!is.na(candidate_comb)) %>%
  ggplot() +
  aes(x = sentiment) +
  geom_histogram(bins = 20L) +
  theme_minimal() +
  ggtitle("Sentiments of all tweets together (data 'large')")  +
  facet_wrap(~ candidate_comb)
```

When comparing the means by candidate, we see that the mean of the sentiments are very close together for all three candidates.

```{r candidate_means}
# mean shows very small differences only between all three candidates
triell_all %>% 
  group_by(candidate) %>% 
  summarise(sent=mean(sentiment), n=n())%>% 
  arrange(desc(n)) %>% 
  arrange(desc(sent))
```

However, when not only distinguishing by dataset (twitter query), but by appearance of the candidates' names, we *do* find some interesting differences. Depending on which candidates are named (alone or together) in the tweets, we get different means for the sentiment scores. Table \ref{tab:all_mean} shows the mean values for the candidates combination of names for the data "all", while table \ref{tab:all_mean} shows the same information for data "large", but with included `NA`, which in this case signifies tweets which have the query "Triell" in the text, but none of the candidates' names.

```{r sentiment_candidate_combined}
triell_all %>% 
  filter(!is.na(candidate_comb)) %>%
  group_by(candidate_comb) %>% 
  summarise(sent=mean(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% 
  head(n=20) %>% 
  arrange(desc(sent))%>%
  knitr::kable(
    caption = "Mean sentiment scores by combination of candidates' names (data 'all')\\label{tab:all_mean}",
  )

triell_large %>% 
#  filter(!is.na(candidate_comb)) %>%
  group_by(candidate_comb) %>% 
  summarise(sent=mean(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% 
  head(n=20) %>% 
  arrange(desc(sent))%>%
  knitr::kable(
  caption = "Mean sentiment scores by combination of candidates' names (data 'large')\\label{tab:large_mean}",
  )
```

### Differences by time of tweet

The sentiments during the Triell can then be plotted over time (in this case by minute).

```{r}
method <- "nrc"
lang <- "german"
ARDtriell$sentiment <- get_sentiment(ARDtriell$text, method=method, language=lang)
ARDtriell$sentiment[1:10]

ggplot(data = ARDtriell, aes(x= time, y= sentiment)) + geom_point() +
  stat_summary(aes(y = sentiment,group=1), fun.y=mean, colour="red", geom="line",group=1)

ARD <- ARDtriell %>%
  filter(candidate_comb %in% c("Baerbock", "Laschet", "Scholz"))
  
ARD %>%
 ggplot(aes(x= time, y= sentiment,  group=candidate_comb ,color=candidate_comb)) +
  geom_line() + theme(legend.position="bottom") + 
  labs(title = "Sentiments in Twitter Debate During ARD Triell", 
       color='Candidates', 
       x = "Time", 
       y = "Sentiment") + 
  scale_color_manual(values = c("green", "black", "red")) + 
  theme(axis.text.x = element_text(size = 3, 
                                   angle = 90), 
        text = element_text(size=10))

# ARD_b <- ARDtriell %>%
#   filter(candidate_comb %in% c("Baerbock"))
#   
# ARD_b %>%
#  ggplot(aes(x= time, y= sentiment,  group=candidate_comb ,color=candidate_comb)) +
#     geom_line()
# 
# ARD_l <- ARDtriell %>%
#   filter(candidate_comb %in% c("Laschet"))
#   
# ARD_l %>%
#  ggplot(aes(x= time, y= sentiment,  group=candidate_comb ,color=candidate_comb)) +
#     geom_line()
# 
# ARD_s <- ARDtriell %>%
#   filter(candidate_comb %in% c("Scholz"))
#   
# ARD_s %>%
#  ggplot(aes(x= time, y= sentiment,  group=candidate_comb ,color=candidate_comb)) +
#     geom_line()
```


## Keyness

We can run a keyness analysis to find differences between the different candidates.

### Differences by candidates' mentions

Running a keyness analysis for the three candidates to check for differences in the keywords. This is first done for the data "all", i.e. where tweets were collected with the queries "Triell AND baerbock" for instance.

```{r keyness_prepare, include = FALSE}
triell_corpus <- corpus(triell_all)

triell_dfm  <-  tokens(triell_corpus) %>% dfm() %>% dfm_group(triell_all$candidate)
```

```{r keyness_exe}
head(textstat_keyness(triell_dfm, target="Baerbock",
                      measure="chi2",sort=T), n=20)

head(textstat_keyness(triell_dfm, target="Scholz",
                      measure="chi2",sort=T), n=20)

head(textstat_keyness(triell_dfm, target="Laschet",
                      measure="chi2",sort=T), n=20)
```

Die häufigsten Wörter der drei Kandidat:innen lassen sich zudem auch als wordcloud darstellen. Nachfolgend die wordcloud von Annalena Baerbock.

```{r keyness_wordcloud_baerbock}
set.seed(1234)
#Package wordcloud
keyness_b <- head(textstat_keyness(triell_dfm, target="Baerbock", measure="chi2",sort=T), n=20)
keyness_b <- keyness_b[-c(1, 2),]#removing "#baerbock" and "baerbock"

#Package wordcloud2
keyness_b_short <- subset(keyness_b, select=c(feature, chi2))
wordcloud2(keyness_b_short, color = "random-dark")
```

Nachfolgend die wordcloud von Annalena Baerbock.

```{r keyness_wordcloud_scholz}
set.seed(1234)
#Wordclouds on keyness analysis Scholz
#Package wordcloud
keyness_s <- head(textstat_keyness(triell_dfm, target="Scholz", measure="chi2",sort=T), n=20)
keyness_s <- keyness_s[-c(1, 2),]#removing "#scholz" and "scholz"

#Package wordcloud2
keyness_s_short <- subset(keyness_s, select=c(feature, chi2))
wordcloud2(keyness_s_short, color = "random-dark")
```

Nachfolgend die wordcloud von Armin Laschet.

```{r keyness_wordcloud_laschet}
set.seed(1234)
#Wordclouds on keyness analysis Laschet
#Package wordcloud
keyness_l <- head(textstat_keyness(triell_dfm, target="Laschet", measure="chi2",sort=T), n=20)
keyness_l <- keyness_l[-c(1, 2),]#removing "#laschet" and "laschet"

#Package wordcloud2
keyness_l_short <- subset(keyness_l, select=c(feature, chi2))
wordcloud2(keyness_l_short, color = "random-dark")
```


## Reaction to tweets: Retweets

### Differences by candidates' mentions

In the following figure we see the number of retweets per tweet by combination of candidates' names.

```{r retweets}
df_tablRet <- triell_all %>% 
  group_by(candidate_comb) %>% 
  summarise(sum = sum(retweet_count)) %>% 
  filter(!is.na(candidate_comb))

ggplot(data = df_tablRet) +
  geom_bar(mapping = aes(x = candidate_comb, y = sum),
           stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))+ 
  labs(title = "Number of retweets by candidate combination",
       x = "Candidate combination", 
       y = "Number of retweets")
```

# Feedback

-   Twitter has a new academic API which gives access to more data
-   Facet plots: Use normalised y-axis, makes comparison easier
-   Language is generally skewed towards the positiv, which is why the values are more on the positive side
-   "Cool group project! You could also compare the mean sentiment for candidates on Twitter with the popularity ratings in the ZDF right after the Triell."
-   Big data can be big on breadth or depth!

