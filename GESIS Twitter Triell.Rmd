---
title: "Twitter communication with candidates for Bundeskanzler:in"
author: Melanie Dietz, Julia L端ck-Benz, Carina Weinmann, Lorenz Biberstein
date: "`r format(Sys.Date(), '%d %b %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
# Global setup for all R chunks in this file
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE) # for now all R chunks are included, can be taken out here later
```

```{r libraries, include = FALSE}
library(rtweet)
library(ggplot2)
library(knitr) # for generating tables in markdown
library(janitor) # for creating tables
library(tm) # F端r text mining
library(tidytext) # for text mining
library(data.table)
library(quanteda) # Quantitative Analysis of Textual Data 
library(quanteda.textstats)
library(syuzhet)
library(dplyr) # For topic modeling
library(topicmodels)
library(udpipe) # for german lemmatization
library(plotly)
library(spacyr) # F端r Lemmatisierung
library(tidyr)
library(stringr) # Working with strings
```

# Introduction

This is a group work of group #2 in the GESIS fall school "Introduction to Computational Social Sciences with Applications in R". 

The aim of this project is to analyse tweets surrounding the election of the Bundeskanzler:in in Germany, focusing on the three candidates Annalena Baerbock (Gr端ne), Olaf Scholz (SPD) and Armin Laschet (CDU).

# Getting twitter data

First, we loaded data from Twitter using the `rtweet` package^[https://cran.r-project.org/web/packages/rtweet/rtweet.pdf]. (Another possibility would be to use the `academictwitteR`^[https://github.com/cjbarrie/academictwitteR, https://cran.r-project.org/web/packages/academictwitteR/index.html], but we could not get it to work in the first tries, so stuck to `rtweet`. 

```{r twitter_token, include = FALSE}
# source("twitter_token.R", local = knitr::knit_global())
# load twitter tokens from separate file (not shared on github for privacy reasons)
```

As two persons from our group already had access to the Twitter API, we collected data twice:

1. Using simply the query "Triell" (no hashtag), collecting as many tweets as possible
2. Using separate queries for the candidates and using a hashtab with "Triell"; e.g. "`#triell AND baerbock`"

Both queries were limited to exclude retweets and only search for German language `include_rts = FALSE, lang = "de"`.

```{r get_tweets_save_large, include = FALSE}
# going for the big guns: trying to download 300k tweets
# 
# triell_large <- rtweet::search_tweets("#Triell", # Search tweets with search phrase "Triell"
#                          include_rts = FALSE, # do not include retweets
#                          lang = "de", # Only tweets from Twitter users with German language
#                          retryonratelimit = TRUE,
#                          n = 300000
#                          )
# 
# # Save data as csv
#  write_as_csv(triell,
#               "data/triell_2021-09-16_large.csv",
#               prepend_ids = TRUE,
#               na = "",
#               fileEncoding = "UTF-8"
#               )

# saveRDS(triell_large, "triell_large.rds")
```

```{r import_csv, include = FALSE}
triell_large <- read.csv("data/triell_2021-09-16_large.csv")

triell_b <- read.csv("data/triell_b_2021-09-15.csv")
triell_l <- read.csv("data/triell_l_2021-09-15.csv")
triell_s <- read.csv("data/triell_s_2021-09-15.csv")
```

The three separate dataframes for the three candidates were then combined together to get one combined dataset called `triell_all` (after adding an identifier variable called `candidate` to distinguish from which query the data comes).

```{r combine_separate_candidates_to_all}
triell_b <- dplyr::mutate(triell_b, candidate = "Baerbock")
triell_l <- dplyr::mutate(triell_l, candidate = "Laschet")
triell_s <- dplyr::mutate(triell_s, candidate = "Scholz")

triell_all <- rbind(triell_b, triell_l, triell_s)
```

This gave us two dataframes; one with the combined searched tweets of the candidates; called `triell_all` of `r count(triell_all)` entries and one large dataframe called `triell_large` of `r count(triell_large)` entries. _We subsequently usually ran our following analyses for both of these datasets as we found it interesting to compare the results based on the slightly different search queries and results (hashtag vs. no hashtab, searching for candidates' names specifically vs. only for "Triell")._

```{r structures, include = FALSE}
str(triell_all)

str(triell_large)
```

# Data cleaning and preparation

<!-- ## Removing punctuation etc. from tweet text -->

<!--
```{r cleaning}
triell_all$text <-  gsub("https\\S*", "", triell_all$text)
triell_all$text <-  gsub("@\\S*", "", triell_all$text) 
triell_all$text <-  gsub("amp", "", triell_all$text) 
triell_all$text <-  gsub("[\r\n]", "", triell_all$text)
# triell_all$text <-  gsub("[[:punct:]]", "", triell_all$text) # remove punctuation

triell_large$text <-  gsub("https\\S*", "", triell_large$text)
triell_large$text <-  gsub("@\\S*", "", triell_large$text) 
triell_large$text <-  gsub("amp", "", triell_large$text) 
triell_large$text <-  gsub("[\r\n]", "", triell_large$text)
# triell_large$text <-  gsub("[[:punct:]]", "", triell_large$text) # remove punctuation
```
-->

## Add candidate identifiers

In the dataset `triell_all` we were able to identify candidates based on the Twitter query (e.g. `#triell AND baerbock`). But as it was possible to have multiple tweets naming not only one but multiple candidated, this identification was not very precise. For this reason, we added another variable searching for each candidate in the text of the TWeet. This gave us three separate variables showing us, whether each of the candidates was present in the tweets (variable `baerbock`, `laschet` and `scholz`).

These three variables were then combined to a new variable called `candidate_comb` to show the combination of the candidates' appearances in the tweets (e.g. Baerbock + Laschet, Baerbock + Scholz + Laschet etc.).

```{r candidate_id}
# Add identifier variable for appearance of candidates in tweets
triell_large <- triell_large %>%
  mutate(baerbock = ifelse(grepl("aerbock", text), 1, 0)) %>%
  mutate(laschet = ifelse(grepl("aschet", text), 10, 0)) %>%
  mutate(scholz = ifelse(grepl("olz", text), 100, 0))

# Combine id variables for all three candidates
triell_large <- triell_large %>%
  mutate(candidate_comb = baerbock + laschet + scholz)

# define labels to combination of appearance of candidates
triell_large$candidate_comb <- factor(triell_large$candidate_comb,
levels = c(1, 10, 100, 11, 101, 110, 111),
labels = c("Baerbock", "Laschet", "Scholz", "Baerbock + Laschet", "Baerbock + Scholz", "Laschet + Scholz", "Baerbock + Scholz + Laschet"))

# do the same for data "triell_all"

# Add identifier variable for appearance of candidates in tweets
triell_all <- triell_all %>%
  mutate(baerbock = ifelse(grepl("aerbock", text), 1, 0)) %>%
  mutate(laschet = ifelse(grepl("aschet", text), 10, 0)) %>%
  mutate(scholz = ifelse(grepl("olz", text), 100, 0))

# Combine id variables for all three candidates
triell_all <- triell_all %>%
  mutate(candidate_comb = baerbock + laschet + scholz)

# define labels to combination of appearance of candidates
triell_all$candidate_comb <- factor(triell_all$candidate_comb,
levels = c(1, 10, 100, 11, 101, 110, 111),
labels = c("Baerbock", "Laschet", "Scholz", "Baerbock + Laschet", "Baerbock + Scholz", "Laschet + Scholz", "Baerbock + Scholz + Laschet"))
```

## Adding dates

<!-- ADD CODE FOR SEPARATING TWEETS BEFORE, DURING AND AFTER SECOND TRIELL FROM JULIA HERE! -->

First we check the first and last dates of our twitter datasets and we see - not surprising - that both dataframes have their newest entries from September 15th (day of download) and oldest entries from September 7th (as you can only access tweets from a certain number of days back via the twitter API).

```{r, include = FALSE}
head(triell_all$created_at)
tail(triell_all$created_at)

head(triell_large$created_at)
tail(triell_large$created_at)
```

Next, we isolated the single days of the `created_at` for later usage.

```{r isolate_days}
triell_all$day <- substr(triell_all$created_at, 1, 10)

triell_large$day <- substr(triell_large$created_at, 1, 10)
```

## Generate corpus & wordlists

For potential later analysis we also created separate objects isolating each word of the tweets, together with the users and tweet time (`created_at`).

```{r generate_wordlist_text}
triell_all_text <- triell_all %>%
  select(name, text, created_at) %>%
  unnest_tokens(word, text)
# new object, only words and names of twitter accounts

triell_large_text <- triell_large %>%
  select(name, text, created_at) %>%
  unnest_tokens(word, text)
# new object, only words and names of twitter accounts
```

Converting the dataframe to a wordlist gives us a new dataframe with `r count(triell_all_text)` entries (for `triell_all`) and `r count (triell_large_text)` (`triell_large`).

```{r, include = FALSE}
str(triell_all_text)

str(triell_large_text)
```

We could also convert the dataframes into corporae, but to properly do this we would need a German lemmatization word list.

```{r}
triell_all_corpus <- corpus(triell_all$text) %>% 
  tokens(.,remove_punct=TRUE,remove_numbers=TRUE,remove_symbols = TRUE) %>% 
  tokens_tolower() # %>% 
# tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed")

triell_large_corpus <- corpus(triell_large$text) %>% 
  tokens(.,remove_punct=TRUE,remove_numbers=TRUE,remove_symbols = TRUE) %>% 
  tokens_tolower() # %>% 
#  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed")
```

```{r, include = FALSE}
# str(triell_all_corpus)
# str(triell_large_corpus)
```

# Data analysis

The aim of the data analysis was to find differences in the tweets, depending on who (which candidate, or which comibination of candidates is mentioned in the tweets) and on the time of sending the tweet.

## Sentiment analysis (using Syuzhet in German)

### Differences by candidates' mentions

For this we get the sentiments of all the tweets, using the `Syuzhet` package, which can also be used with German (setting `lang <- "german"`)^[Although we do not know how precise this is.].

```{r create_sentiments}
# path_to_a_text_file <- system.file("extdata", "quijote.txt",package = "syuzhet")
# my_text <- get_text_as_string(path_to_a_text_file)
# char_v <- get_sentences(triell_all$text)
method <- "nrc"
lang <- "german"
triell_all$sentiment <- get_sentiment(triell_all$text, method=method, language=lang)
triell_all$sentiment[1:10]

triell_large$sentiment <- get_sentiment(triell_large$text, method=method, language=lang)
triell_large$sentiment[1:10]
```

```{r sentiment_histo}
triell_all %>%
 ggplot() +
  aes(x = sentiment) +
  geom_histogram(bins = 30L) +
  theme_minimal() +
  ggtitle("Sentiments of all tweets together, by candidate")  +
  facet_wrap(~ candidate)

triell_large %>%
 ggplot() +
  aes(x = sentiment) +
  geom_histogram(bins = 30L) +
  theme_minimal() +
  ggtitle("Sentiments of all tweets together (large dataset)")
```

```{r sentiment_candidate}
# mean shows very small differences only between all three candidates
triell_all %>% 
  group_by(candidate) %>% 
  summarise(sent=mean(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% 
  head(n=20) %>% 
  arrange(desc(sent))

# median confirms this; all three have a median of zero
triell_all %>% 
  group_by(candidate) %>% 
  summarise(sent=median(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% 
  head(n=20) %>% 
  arrange(desc(sent))
```

```{r sentiment_candidate_combined}
triell_all %>% 
#  filter(!is.na(candidate_comb)) %>%
  group_by(candidate_comb) %>% 
  summarise(sent=mean(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% 
  head(n=20) %>% 
  arrange(desc(sent))

triell_large %>% 
#  filter(!is.na(candidate_comb)) %>%
  group_by(candidate_comb) %>% 
  summarise(sent=mean(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% 
  head(n=20) %>% 
  arrange(desc(sent))
```

### Differences by time of tweet

## Keyness

### Differences by candidates' mentions

Running a keyness analysis for the three candidates.

```{r}
triell_corpus <- corpus(triell_all)

triell_dfm  <-  tokens(triell_corpus) %>% dfm() %>% dfm_group(triell_all$candidate)

head(textstat_keyness(triell_dfm, target="Baerbock",
                      measure="chi2",sort=T), n=20)

head(textstat_keyness(triell_dfm, target="Scholz",
                      measure="chi2",sort=T), n=20)

head(textstat_keyness(triell_dfm, target="Laschet",
                      measure="chi2",sort=T), n=20)
```

### Differences by time of tweet

## Reaction to tweets: Retweets and likes

### Differences by candidates' mentions

### Differences by time of tweet

## Who tweets what? Analysing sender of tweets

### Differences by candidates' mentions

### Differences by time of tweet



