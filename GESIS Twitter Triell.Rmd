---
title: "Twitter communication after Triell"
author: Melanie Dietz, Julia Lück-Benz, Carina Weinmann, Lorenz Biberstein
date: "`r format(Sys.Date(), '%d %b %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
# Global setup for all R chunks in this file
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(include = TRUE) # for now all R chunks are included, can be taken out here later
```

```{r libraries}
library(rtweet)
library(ggplot2)
library(knitr) # for generating tables in markdown
library(janitor) # for creating tables
library(tm) # Für text mining
library(tidytext) # for text mining
library(data.table)
library(quanteda) # Quantitative Analysis of Textual Data 
library(quanteda.textstats)
library(syuzhet)
library(dplyr) # For topic modeling
library(topicmodels)
library(udpipe) # for german lemmatization
library(plotly)
library(spacyr) # Für Lemmatisierung
library(tidyr)
library(stringr) # Working with strings
```


# Introduction

This is a group work of group #2 in the GESIS fall school "Introduction to Computational Social Sciences with Applications in R". 

The aim of this work is to analyse tweets after the first and the second Triell^[Triell is a portmanteau of "tri", i.e. "three" and "Duell".] in Germany (podium discussions before the "Bundestagswahl").

# Getting twitter data

First, we load data from Twitter. Data from twitter is loaded using the `rtweet` package^[https://cran.r-project.org/web/packages/rtweet/rtweet.pdf] or the `academictwitteR` package^[https://github.com/cjbarrie/academictwitteR, https://cran.r-project.org/web/packages/academictwitteR/index.html]. 

```{r twitter_token}
# source("twitter_token.R", local = knitr::knit_global())
# load twitter tokens from separate file (not shared on github for privacy reasons)
```

We search for tweets with the query "Triell", do not include retweets and only search for tweets from users with German language.

```{r get_tweets_save_large}
# going for the big guns: trying to download 300k tweets
# 
# triell_large <- rtweet::search_tweets("#Triell", # Search tweets with search phrase "Triell"
#                          include_rts = FALSE, # do not include retweets
#                          lang = "de", # Only tweets from Twitter users with German language
#                          retryonratelimit = TRUE,
#                          n = 300000
#                          )
# 
# # Save data as csv
#  write_as_csv(triell,
#               "data/triell_2021-09-16_large.csv",
#               prepend_ids = TRUE,
#               na = "",
#               fileEncoding = "UTF-8"
#               )

# saveRDS(triell_large, "triell_large.rds")
```

```{r import_csv}
triell_large <- read.csv("data/triell_2021-09-16_large.csv")

triell_b <- read.csv("data/triell_b_2021-09-15.csv")
triell_l <- read.csv("data/triell_l_2021-09-15.csv")
triell_s <- read.csv("data/triell_s_2021-09-15.csv")
```


```{r combine_separate_candidates_to_all}
triell_b <- dplyr::mutate(triell_b, candidate = "Baerbock")
triell_l <- dplyr::mutate(triell_l, candidate = "Laschet")
triell_s <- dplyr::mutate(triell_s, candidate = "Scholz")

triell_all <- rbind(triell_b, triell_l, triell_s)
```

This gives us two dataframes; one with the combined searched tweets of the candidates; called `triell_all` of `r count(triell_all)` entries and one large dataframe called `triell_large` of `r count(triell_large)` entries.

```{r}
str(triell_all)
```

# Data cleaning and preparation

## Removing punctuation etc. from tweet text

```{r cleaning}
triell_all$text <-  gsub("https\\S*", "", triell_all$text)
triell_all$text <-  gsub("@\\S*", "", triell_all$text) 
triell_all$text <-  gsub("amp", "", triell_all$text) 
triell_all$text <-  gsub("[\r\n]", "", triell_all$text)
triell_all$text <-  gsub("[[:punct:]]", "", triell_all$text) # remove punctuation

triell_large$text <-  gsub("https\\S*", "", triell_large$text)
triell_large$text <-  gsub("@\\S*", "", triell_large$text) 
triell_large$text <-  gsub("amp", "", triell_large$text) 
triell_large$text <-  gsub("[\r\n]", "", triell_large$text)
triell_large$text <-  gsub("[[:punct:]]", "", triell_large$text) # remove punctuation
```

## Add candidate identifiers

```{r candidate_id}
# Add identifier variable for appearance of candidates in tweets
triell_large <- triell_large %>%
  mutate(baerbock = ifelse(grepl("aerbock", text), 1, 0)) %>%
  mutate(laschet = ifelse(grepl("aschet", text), 1, 0)) %>%
  mutate(scholz = ifelse(grepl("olz", text), 1, 0))
```

## Adding dates

First we check the first and last dates of our twitter datasets and we see - not surprising - that both dataframes have their newest entries from September 15th (day of download) and oldest entries from September 7th (as you can only access tweets from a certain number of days back via the twitter API).

```{r}
head(triell_all$created_at)
tail(triell_all$created_at)

head(triell_large$created_at)
tail(triell_large$created_at)
```

Next, we can isolate the single days of the `created_at` for later usage.

```{r isolate_days}
triell_all$day <- substr(triell_all$created_at, 1, 10)

triell_large$day <- substr(triell_large$created_at, 1, 10)
```

## Generate corpus & wordlists

```{r generate_wordlist_text}
triell_all_text <- triell_all %>%
  select(name, text, created_at) %>%
  unnest_tokens(word, text)
# new object, only words and names of twitter accounts

triell_large_text <- triell_large %>%
  select(name, text, created_at) %>%
  unnest_tokens(word, text)
# new object, only words and names of twitter accounts
```

Converting the dataframe to a wordlist gives us a new dataframe with `r count(triell_text)` entries (for `triell_all`) and `r count (triell_large)` (`triell_large`).

```{r}
str(triell_all_text)

str(triell_large_text)
```

We can also convert the `triell` dataframes into a corpus.

```{r}
triell_all_corpus <- corpus(triell_all$text) %>% tokens(.,remove_punct=TRUE,remove_numbers=TRUE,remove_symbols = TRUE) %>% tokens_tolower() %>% tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed")

triell_large_corpus <- corpus(triell_large$text) %>% tokens(.,remove_punct=TRUE,remove_numbers=TRUE,remove_symbols = TRUE) %>% tokens_tolower() %>% tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed")
```

```{r}
str(triell_all_corpus)
str(triell_large_corpus)
```

# Data analysis

Our aim is to analyse the German twitter sphere after the first and the second Triell to find differences in the tweets (sentiment analysis etc.).

## Sentiment analysis?

### Syuzhet in German

```{r syzhet}
# path_to_a_text_file <- system.file("extdata", "quijote.txt",package = "syuzhet")
# my_text <- get_text_as_string(path_to_a_text_file)
# char_v <- get_sentences(triell_all$text)
method <- "nrc"
lang <- "german"
triell_all$sentiment <- get_sentiment(triell_all$text, method=method, language=lang)
triell_all$sentiment[1:10]
```


```{r sentiment_candidate}
triell_all %>% 
  group_by(candidate) %>% 
  summarise(sent=mean(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% head(n=20) %>% 
  arrange(desc(sent))
```


